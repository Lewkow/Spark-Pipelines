{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark\n",
    "\n",
    "## Introduction to Resiliant Distributed Datasets (RDD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "\n",
    "* Spark is a distributed computing framework for big data. \n",
    "* Clusters of worker machines operate on Resiliant Distributed Datasets (RDDs) in paralell. \n",
    "* Spark is optimized to utilized worker memory as much as possible, allowing for fast computation on large datasets.\n",
    "\n",
    "<img src=\"figs/RDD_1.png\">\n",
    "#### A spark driver, either through an application or shell, connects to a cluster consisting of worker machines. (Databricks Workshop 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "* RDDs are collections of data either from a central source which is then parallelized (file on local machine) or from a distributed data store (HDFS, Cassandra etc).\n",
    "* RDDs can consist of common data types and arbitrary objects.\n",
    "* An RDD is split between several partitions and distributed to the different worker machines.\n",
    "<img src=\"figs/RDD_2.png\">\n",
    "#### Large dataset partitioned between different worker machines in a Spark cluster (Databricks Workshop 2015) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "* Example RDD consisting of logging data\n",
    "\n",
    "<img src=\"figs/RDD_3.png\">\n",
    "#### (Databricks Workshop 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "* Creating an RDD from a collection of data on the driver and distributing them to the worker machines\n",
    "\n",
    "<img src=\"figs/RDD_4.png\">\n",
    "#### (Databricks Workshop 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"testapp\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"testapp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195\n",
      "3\n",
      "fish\n",
      "['fish', 'cats', 'dogs']\n"
     ]
    }
   ],
   "source": [
    "# Create RDD from a python list\n",
    "wordsRDD = sc.parallelize([\"fish\", \"cats\", \"dogs\"])\n",
    "\n",
    "# View object type\n",
    "print(wordsRDD)\n",
    "\n",
    "# Get number of objects in RDD\n",
    "print(wordsRDD.count())\n",
    "\n",
    "# Get the first object in the RDD\n",
    "print(wordsRDD.first())\n",
    "\n",
    "# Collect all the objects in the RDD from Spark back to driver (this notebook)\n",
    "print(wordsRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "* Creating an RDD from a file\n",
    "  * Can come from local machine (small files)\n",
    "  * Can read files directly from HDFS and Cassandra\n",
    "    * example: > newRDD = sc.textFile(\"hdfs://localhost:9000/user/data/file.txt\")\n",
    "\n",
    "<img src=\"figs/RDD_5.png\">\n",
    "#### (Databricks Workshop 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a total of 22933 lines in Moby Dick\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['MOBY DICK; OR THE WHALE ',\n",
       " '',\n",
       " 'by Herman Melville',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'ETYMOLOGY.',\n",
       " '',\n",
       " '(Supplied by a Late Consumptive Usher to a Grammar School)',\n",
       " '',\n",
       " 'The pale Usher--threadbare in coat, heart, body, and brain; I see him',\n",
       " 'now.  He was ever dusting his old lexicons and grammars, with a queer',\n",
       " 'handkerchief, mockingly embellished with all the gay flags of all the',\n",
       " 'known nations of the world.  He loved to dust his old grammars; it',\n",
       " 'somehow mildly reminded him of his mortality.',\n",
       " '',\n",
       " '\"While you take in hand to school others, and to teach them by what',\n",
       " 'name a whale-fish is to be called in our tongue leaving out, through',\n",
       " 'ignorance, the letter H, which almost alone maketh the signification',\n",
       " 'of the word, you deliver that which is not true.\" --HACKLUYT',\n",
       " '',\n",
       " '\"WHALE. ... Sw. and Dan. HVAL.  This animal is named from roundness',\n",
       " 'or rolling; for in Dan. HVALT is arched or vaulted.\" --WEBSTER\\'S',\n",
       " 'DICTIONARY',\n",
       " '',\n",
       " '\"WHALE. ... It is more immediately from the Dut. and Ger. WALLEN;',\n",
       " 'A.S. WALW-IAN, to roll, to wallow.\" --RICHARDSON\\'S DICTIONARY',\n",
       " '',\n",
       " 'KETOS,               GREEK.',\n",
       " 'CETUS,               LATIN.',\n",
       " 'WHOEL,               ANGLO-SAXON.',\n",
       " 'HVALT,               DANISH.',\n",
       " 'WAL,                 DUTCH.',\n",
       " 'HWAL,                SWEDISH.',\n",
       " 'WHALE,               ICELANDIC.',\n",
       " 'WHALE,               ENGLISH.',\n",
       " 'BALEINE,             FRENCH.',\n",
       " 'BALLENA,             SPANISH.',\n",
       " 'PEKEE-NUEE-NUEE,     FEGEE.',\n",
       " 'PEKEE-NUEE-NUEE,     ERROMANGOAN.',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'EXTRACTS (Supplied by a Sub-Sub-Librarian).',\n",
       " '',\n",
       " 'It will be seen that this mere painstaking burrower and grub-worm of',\n",
       " 'a poor devil of a Sub-Sub appears to have gone through the long',\n",
       " 'Vaticans and street-stalls of the earth, picking up whatever random']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in Moby Dick text file (reads in one line per RDD entry)\n",
    "mobyDickRDD = sc.textFile(\"./data/MobyDick.txt\")\n",
    "\n",
    "# Count how many lines are in RDD\n",
    "print(\"There is a total of %d lines in Moby Dick\" % mobyDickRDD.count())\n",
    "\n",
    "# Take first 5 lines from RDD\n",
    "mobyDickRDD.take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "* Typical workflow\n",
    "  * Data is read from external sources (HDFS, Cassandra, local file...) into an RDD\n",
    "  * Transformations are performed on RDD to create new RDDs\n",
    "    * In this example, the main RDD is filtered to create an RDD which only contains Error messages\n",
    "\n",
    "<img src=\"figs/RDD_6.png\">\n",
    "#### (Databricks Workshop 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "* Filtered RDDs can be coalesced to reduce the number of partitions which make up that RDD\n",
    "  * Used when filtered RDDs become sparse compared to their parent RDD\n",
    "\n",
    "<img src=\"figs/RDD_7.png\">\n",
    "#### (Databricks Workshop 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# The number of paritions for a RDD can be specified when RDD is created\n",
    "# This is automatically done if no parameter is passed\n",
    "N_partitions = 4\n",
    "team = sc.parallelize([\"Al\", \"Ani\", \"Jackie\", \"Lalitha\", \"Mark\", \"Neil\", \"Nick\", \"Shirin\"], N_partitions)\n",
    "print(team.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "* Standard workflow for a Spark application\n",
    "\n",
    "<img src=\"figs/RDD_8.png\">\n",
    "#### (Databricks Workshop 2015)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
